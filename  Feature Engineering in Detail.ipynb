{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Features are allocated scores and can then be ranked by their scores. Those features with the highest scores can be selected for inclusion in the training dataset, whereas those remaining can be ignored.\n",
    "\n",
    "1. In computer vision, an image is an observation, but a feature could be a line in the image. In natural language processing, a document or a tweet could be an observation, and a phrase or word count could be a feature. In speech recognition, an utterance could be an observation, but a feature might be a single word or phoneme.\n",
    "\n",
    "2. More complex predictive modeling algorithms perform feature importance and selection internally while constructing their model. Some examples include MARS, Random Forest and Gradient Boosted Machines. These models can also report on the variable importance determined during the model preparation process.\n",
    "\n",
    "Feature extraction is a process of automatically reducing the dimensionality of these types of observations into a much smaller set that can be modelled.\n",
    "\n",
    "1. For tabular data, this might include projection methods like Principal Component Analysis and unsupervised clustering methods. For image data, this might include line or edge detection. Depending on the domain, image, video and audio observations lend themselves to many of the same types of DSP methods.\n",
    "\n",
    "2. Key to feature extraction is that the methods are automatic (although may need to be designed and constructed from simpler methods) and solve the problem of unmanageably high dimensional data, most typically used for analog observations stored in digital formats.\n",
    "\n",
    "A picture relevant to our discussion on feature engineering is the front-middle of this process. It might look something like the following:\n",
    "\n",
    "1. (tasks before here…)\n",
    "2. Select Data: Integrate data, de-normalize it into a dataset, collect it together.\n",
    "3. Preprocess Data: Format it, clean it, sample it so you can work with it.\n",
    "4. Transform Data: Feature Engineer happens here.\n",
    "5. Model Data: Create models, evaluate them and tune them.\n",
    "6. (tasks after here…)\n",
    "The traditional idea of “Transforming Data” from a raw state to a state suitable for modeling is where feature engineering fits in. Transform data and feature engineering may in fact be synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notable approaches\n",
    "A. Feature selection using regularization:\n",
    "– L0 regularisation\n",
    "– L1 regularisation \n",
    "– L2 regularisation\n",
    "– L1/2 regularization\n",
    "\n",
    "B. Feature selection using wrappers.\n",
    "1.  Wrappers:\n",
    "– Assume we have chosen a learning system and algorithm.\n",
    "– Navigate feature subsets by adding/removing features.\n",
    "– Evaluate on the validation set.\n",
    "\n",
    "2. Backward selection wrapper:\n",
    "– Start with all features.\n",
    "– Try removing each feature and measure validation set impact.\n",
    "– Remove the feature that causes the least harm.\n",
    "– Repeat.\n",
    "\n",
    "3. Notes:\n",
    "– There are many variants (forward, backtracking, etc.)\n",
    "– Risk of overfitting the validation set.\n",
    "– Computationally expensive.\n",
    "– Quite effective in practice.\n",
    "\n",
    "C. Feature selection using greedy algorithms.\n",
    "Algorithms that incorporate features one by one.\n",
    "1. Decision trees: \n",
    "– Each decision can be seen as a feature.\n",
    "– Pruning the decision tree prunes the features\n",
    "\n",
    "2. Ensembles: \n",
    "– Ensembles of classifiers involving few features.\n",
    "– Random forests.\n",
    "– Boosting.\n",
    "\n",
    "D. Filter approaches: \n",
    "1. Compute some measure for estimating the ability to discriminate between classes\n",
    "2. Typically measure feature weight and select the best n features →supervised ranked feature selection\n",
    "\n",
    "Problems:\n",
    "\n",
    "-Redundant features (correlated features will all have similar weights)\n",
    "\n",
    "-Dependant features (some features may only be important in\n",
    "combination)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Data Preparation\n",
    "Hi, in this lesson, you will discover the importance of data preparation in predictive modeling with machine learning.\n",
    "\n",
    "Predictive modeling projects involve learning from data.\n",
    "Data refers to examples or cases from the domain that characterizes the problem you want to solve.\n",
    "On a predictive modeling project, such as classification or regression, raw data typically cannot be used directly.\n",
    "There are four main reasons why this is the case:\n",
    "Data Types: Machine learning algorithms require data to be numbers.\n",
    "Data Requirements: Some machine learning algorithms impose requirements on the data.\n",
    "Data Errors: Statistical noise and errors in the data may need to be corrected.\n",
    "Data Complexity: Complex nonlinear relationships may be teased out of the data.\n",
    "The raw data must be pre-processed prior to being used to fit and evaluate a machine learning model. This step in a predictive modeling project is referred to as data preparation.\n",
    "There are common or standard tasks that you may use or explore during the data preparation step in a machine learning project.\n",
    "These tasks include:\n",
    "1. Data Cleaning: Identifying and correcting mistakes or errors in the data.\n",
    "2. Feature Selection: Identifying those input variables that are most relevant to the task.\n",
    "3. Data Transforms: Changing the scale or distribution of variables.\n",
    "4. Feature Engineering: Deriving new variables from available data.\n",
    "5. Dimensionality Reduction: Creating compact projections of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Missing Values With Imputation\n",
    "Data can have missing values for a number of reasons, such as observations that were not recorded and data corruption. Handling missing data is important as many machine learning algorithms do not support data with missing values.\n",
    "\n",
    "Filling missing values with data is called data imputation and a popular approach for data imputation is to calculate a statistical value for each column (such as a mean) and replace all missing values for that column with the statistic.\n",
    "\n",
    "The horse colic dataset describes the medical characteristics of horses with colic and whether they lived or died. It has missing values marked with a question mark '?'. We can load the dataset with the read_csv() function and ensure that question mark values are marked as NaN.\n",
    "\n",
    "Once loaded, we can use the SimpleImputer class to transform all missing values marked with a NaN value with the mean of the column.\n",
    "\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n",
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "# statistical imputation transform for the horse colic dataset\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "dataframe = read_csv(url, header=None, na_values='?')\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Features With RFE\n",
    "Hi, In this lesson, you will discover how to select the most important features in a dataset.\n",
    "\n",
    "Feature selection is the process of reducing the number of input variables when developing a predictive model.\n",
    "\n",
    "It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model.\n",
    "Recursive Feature Elimination, or RFE for short, is a popular feature selection algorithm.\n",
    "\n",
    "RFE is popular because it is easy to configure and use and because it is effective at selecting those features (columns) in a training dataset that are more or most relevant in predicting the target variable.\n",
    "\n",
    "The scikit-learn Python machine learning library provides an implementation of RFE for machine learning. RFE is a transform. To use it, first, the class is configured with the chosen algorithm specified via the estimator argument and the number of features to select via the n_features_to_select argument.\n",
    "\n",
    "The example below defines a synthetic classification dataset with five redundant input features. RFE is then used to select five features using the decision tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected=False, Rank: 4\n",
      "Column: 1, Selected=False, Rank: 5\n",
      "Column: 2, Selected=True, Rank: 1\n",
      "Column: 3, Selected=True, Rank: 1\n",
      "Column: 4, Selected=True, Rank: 1\n",
      "Column: 5, Selected=False, Rank: 6\n",
      "Column: 6, Selected=True, Rank: 1\n",
      "Column: 7, Selected=False, Rank: 3\n",
      "Column: 8, Selected=True, Rank: 1\n",
      "Column: 9, Selected=False, Rank: 2\n"
     ]
    }
   ],
   "source": [
    "# report which features were selected by RFE\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# define RFE\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "# summarize all features\n",
    "for i in range(X.shape[1]):\n",
    "    print('Column: %d, Selected=%s, Rank: %d' % (i, rfe.support_[i], rfe.ranking_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Data With Normalization\n",
    "Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n",
    "\n",
    "This includes algorithms that use a weighted sum of the input, like linear regression and algorithms that use distance measures, like k-nearest neighbors.\n",
    "\n",
    "One of the most popular techniques for scaling numerical data prior to modeling is normalization. Normalization scales each input variable separately to the range 0-1, which is the range for floating-point values where we have the most precision. It requires that you know or are able to accurately estimate the minimum and maximum observable values for each variable. You may be able to estimate these values from your available data.\n",
    "\n",
    "You can normalize your dataset using the scikit-learn object MinMaxScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.39324489 -5.77732048 -0.59062319 -2.08095322  1.04707034]\n",
      " [-0.45820294  1.94683482 -2.46471441  2.36590955 -0.73666725]\n",
      " [ 2.35162422 -1.00061698 -0.5946091   1.12531096 -0.65267587]]\n",
      "[[0.77608466 0.0239289  0.48251588 0.18352101 0.59830036]\n",
      " [0.40400165 0.79590304 0.27369632 0.6331332  0.42104156]\n",
      " [0.77065362 0.50132629 0.48207176 0.5076991  0.4293882 ]]\n"
     ]
    }
   ],
   "source": [
    "# example of normalizing input data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=5, n_redundant=0, random_state=1)\n",
    "# summarize data before the transform\n",
    "print(X[:3, :])\n",
    "# define the scaler\n",
    "trans = MinMaxScaler()\n",
    "# transform the data\n",
    "X_norm = trans.fit_transform(X)\n",
    "# summarize data after the transform\n",
    "print(X_norm[:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Categories With One Hot Encoding\n",
    "Machine learning models require all input and output variables to be numeric. This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model.\n",
    "\n",
    "One of the most popular techniques for transforming categorical variables into numbers is the one hot encoding.\n",
    "\n",
    "Categorical data are variables that contain label values rather than numeric values.\n",
    "\n",
    "Each label for a categorical variable can be mapped to a unique integer, called an ordinal encoding. Then, a one hot encoding can be applied to the ordinal representation. This is where one new binary variable is added to the dataset for each unique integer value in the variable, and the original categorical variable is removed from the dataset.\n",
    "\n",
    "For example, imagine we have a color variable with three categories (red, green, and blue). In this case, three binary variables are needed. A \"1\" value is placed in the binary variable for the color and \"0\" values for the other colors.\n",
    "\n",
    "This one hot encoding transform is available in the scikit-learn Python machine learning library via the OneHotEncoder class.\n",
    "\n",
    "The breast cancer dataset contains only categorical input variables.\n",
    "The example below loads the dataset and one hot encodes each of the categorical input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"'40-49'\" \"'premeno'\" \"'15-19'\" \"'0-2'\" \"'yes'\" \"'3'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'15-19'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\" \"'central'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'35-39'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# one hot encode the breast cancer dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define the location of the dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "# load the dataset\n",
    "dataset = read_csv(url, header=None)\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:, -1].astype(str)\n",
    "# summarize the raw data\n",
    "print(X[:3, :])\n",
    "# define the one hot encoding transform\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "# fit and apply the transform to the input data\n",
    "X_oe = encoder.fit_transform(X)\n",
    "# summarize the transformed data\n",
    "print(X_oe[:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Numbers to Categories With kBins\n",
    "Hi, in this lesson, you will discover how to transform numerical variables into categorical variables.\n",
    "\n",
    "Some machine learning algorithms may prefer or require categorical or ordinal input variables, such as some decision tree and rule-based algorithms.\n",
    "\n",
    "This could be caused by outliers in the data, multi-modal distributions, highly exponential distributions, and more.\n",
    "\n",
    "Many machine learning algorithms prefer or perform better when numerical input variables with non-standard distributions are transformed to have a new distribution or an entirely new data type.\n",
    "\n",
    "One approach is to use the transform of the numerical variable to have a discrete probability distribution where each numerical value is assigned a label and the labels have an ordered (ordinal) relationship.\n",
    "\n",
    "This is called a discretization transform and can improve the performance of some machine learning models for datasets by making the probability distribution of numerical input variables discrete.\n",
    "\n",
    "The discretization transform is available in the scikit-learn Python machine learning library via the KBinsDiscretizer class.\n",
    "\n",
    "It allows you to specify the number of discrete bins to create (n_bins), whether the result of the transform will be an ordinal or one hot encoding (encode), and the distribution used to divide up the values of the variable (strategy), such as 'uniform'.\n",
    "\n",
    "The example below creates a synthetic input variable with 10 numerical input variables, then encodes each into 10 discrete bins with an ordinal encoding\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.39324489 -5.77732048 -0.59062319 -2.08095322  1.04707034]\n",
      " [-0.45820294  1.94683482 -2.46471441  2.36590955 -0.73666725]\n",
      " [ 2.35162422 -1.00061698 -0.5946091   1.12531096 -0.65267587]]\n",
      "[[7. 0. 4. 1. 5.]\n",
      " [4. 7. 2. 6. 4.]\n",
      " [7. 5. 4. 5. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# discretize numeric input variables\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=5, n_redundant=0, random_state=1)\n",
    "# summarize data before the transform\n",
    "print(X[:3, :])\n",
    "# define the transform\n",
    "trans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "# transform the data\n",
    "X_discrete = trans.fit_transform(X)\n",
    "# summarize data after the transform\n",
    "print(X_discrete[:3, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction With PCA\n",
    "Hi, in this lesson, you will discover how to use dimensionality reduction to reduce the number of input variables in a dataset.\n",
    "\n",
    "The number of input variables or features for a dataset is referred to as its dimensionality.\n",
    "\n",
    "Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset.\n",
    "\n",
    "More input features often make a predictive modeling task more challenging to model, more generally referred to as the curse of dimensionality.\n",
    "\n",
    "Although on high-dimensionality statistics, dimensionality reduction techniques are often used for data visualization, these techniques can be used in applied machine learning to simplify a classification or regression dataset in order to better fit a predictive model.\n",
    "\n",
    "Perhaps the most popular technique for dimensionality reduction in machine learning is Principal Component Analysis, or PCA for short. This is a technique that comes from the field of linear algebra and can be used as a data preparation technique to create a projection of a dataset prior to fitting a model.\n",
    "\n",
    "The resulting dataset, the projection, can then be used as input to train a machine learning model.\n",
    "\n",
    "The scikit-learn library provides the PCA class that can be fit on a dataset and used to transform a training dataset and any additional datasets in the future.\n",
    "\n",
    "The example below creates a synthetic binary classification dataset with 10 input variables then uses PCA to reduce the dimensionality of the dataset to the three most important components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.53448246  0.93837451  0.38969914  0.0926655   1.70876508  1.14351305\n",
      "  -1.47034214  0.11857673 -2.72241741  0.2953565 ]\n",
      " [-2.42280473 -1.02658758 -2.34792156 -0.82422408  0.59933419 -2.44832253\n",
      "   0.39750207  2.0265065   1.83374105  0.72430365]\n",
      " [-1.83391794 -1.1946668  -0.73806871  1.50947233  1.78047734  0.58779205\n",
      "  -2.78506977 -0.04163788 -1.25227833  0.99373587]]\n",
      "[[-1.64710578 -2.11683302  1.98256096]\n",
      " [ 0.92840209  4.8294997   0.22727043]\n",
      " [-3.83677757  0.32300714  0.11512801]]\n"
     ]
    }
   ],
   "source": [
    "# example of pca for dimensionality reduction\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=7, random_state=1)\n",
    "# summarize data before the transform\n",
    "print(X[:3, :])\n",
    "# define the transform\n",
    "trans = PCA(n_components=3)\n",
    "# transform the data\n",
    "X_dim = trans.fit_transform(X)\n",
    "# summarize data after the transform\n",
    "print(X_dim[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
